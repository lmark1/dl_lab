{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DU_LAB2.ipynb","version":"0.3.2","views":{},"default_view":{},"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"umwLTBNchTTU","colab_type":"code","colab":{"autoexec":{"startup":false,"wait_interval":0},"base_uri":"https://localhost:8080/","height":375},"outputId":"43ed0eba-3a9c-4da0-82fa-f35ed32b83b8","executionInfo":{"status":"error","timestamp":1523542280880,"user_tz":-120,"elapsed":1708,"user":{"displayName":"Lovro MarkoviÄ‡","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s128","userId":"100715975887457631561"}}},"cell_type":"code","source":["from abc import ABCMeta, abstractmethod\n","import numpy as np\n","import scipy\n","import scipy.stats as stats\n","\n","import pyximport\n","pyximport.install()\n","from im2col_cython import col2im_cython, im2col_cython\n","zero_init = np.zeros\n","\n","def variance_scaling_initializer(shape, fan_in, factor=2.0, seed=None):\n","  sigma = np.sqrt(factor / fan_in)\n","  return stats.truncnorm(-2, 2, loc=0, scale=sigma).rvs(shape)\n","\n","\n","# -- ABSTRACT CLASS DEFINITION --\n","class Layer(metaclass = ABCMeta):\n","  \"Interface for layers\"\n","  # See documentation of abstract base classes (ABC): https://docs.python.org/3/library/abc.html\n","\n","  @abstractmethod\n","  def forward(self, inputs):\n","    \"\"\"\n","    Args:\n","      inputs: ndarray tensor.\n","    Returns:\n","      ndarray tensor, result of the forward pass.\n","    \"\"\"\n","    pass\n","\n","  @abstractmethod\n","  def backward_inputs(self, grads):\n","    \"\"\"\n","    Args:\n","      grads: gradient of the loss with respect to the output of the layer.\n","    Returns:\n","      Gradient of the loss with respect to the input of the layer.\n","    \"\"\"\n","    pass\n","\n","  def backward_params(self, grads):\n","    \"\"\"\n","    Args:\n","      grads: gradient of the loss with respect to the output of the layer.\n","    Returns:\n","      Gradient of the loss with respect to all the parameters of the layer as a list\n","      [[w0, g0], ..., [wk, gk], self.name] where w are parameter weights and g their gradient.\n","      Note that wk and gk must have the same shape.\n","    \"\"\"\n","    pass\n","\n","\n","# -- CONVOLUTION LAYER --\n","class Convolution(Layer):\n","  \"N-dimensional convolution layer\"\n","\n","  def __init__(self, input_layer, num_filters, kernel_size, name, padding='SAME',\n","               weights_initializer_fn=variance_scaling_initializer,\n","               bias_initializer_fn=zero_init):\n","    self.input_shape = input_layer.shape\n","    N, C, H, W = input_layer.shape\n","    self.C = C\n","    self.N = N\n","    self.num_filters = num_filters\n","    self.kernel_size = kernel_size\n","\n","    assert kernel_size % 2 == 1\n","\n","    self.padding = padding\n","    if padding == 'SAME':\n","      # with zero padding\n","      self.shape = (N, num_filters, H, W)\n","      self.pad = (kernel_size - 1) // 2\n","    else:\n","      # without padding\n","      self.shape = (N, num_filters, H - kernel_size + 1, W - kernel_size + 1)\n","      self.pad = 0\n","\n","    fan_in = C * kernel_size**2\n","    self.weights = weights_initializer_fn([num_filters, kernel_size**2 * C], fan_in)\n","    self.bias = bias_initializer_fn([num_filters])\n","    # this implementation doesn't support strided convolutions\n","    self.stride = 1\n","    self.name = name\n","    self.has_params = True\n","\n","  def forward(self, x):\n","    k = self.kernel_size\n","    self.x_cols = im2col_cython(x, k, k, self.pad, self.stride)\n","    res = self.weights.dot(self.x_cols) + self.bias.reshape(-1, 1)\n","    N, C, H, W = x.shape\n","    out = res.reshape(self.num_filters, self.shape[2], self.shape[3], N)\n","    return out.transpose(3, 0, 1, 2)\n","\n","  def backward_inputs(self, grad_out):\n","    # nice trick from CS231n, backward pass can be done with just matrix mul and col2im\n","    grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n","    grad_x_cols = self.weights.T.dot(grad_out)\n","    N, C, H, W = self.input_shape\n","    k = self.kernel_size\n","    grad_x = col2im_cython(grad_x_cols, N, C, H, W, k, k, self.pad, self.stride)\n","    return grad_x\n","\n","  def backward_params(self, grad_out):\n","    grad_bias = np.sum(grad_out, axis=(0, 2, 3))\n","    grad_out = grad_out.transpose(1, 2, 3, 0).reshape(self.num_filters, -1)\n","    grad_weights = grad_out.dot(self.x_cols.T).reshape(self.weights.shape)\n","    return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n","\n","\n","class MaxPooling(Layer):\n","  def __init__(self, input_layer, name, pool_size=2, stride=2):\n","    self.name = name\n","    self.input_shape = input_layer.shape\n","    N, C, H, W = self.input_shape\n","    self.stride = stride\n","    self.shape = (N, C, H // stride, W // stride)\n","    self.pool_size = pool_size\n","    assert pool_size == stride, 'Invalid pooling params'\n","    assert H % pool_size == 0\n","    assert W % pool_size == 0\n","    self.has_params = False\n","\n","  def forward(self, x):\n","    N, C, H, W = x.shape\n","    self.input_shape = x.shape\n","    # with this clever reshaping we can implement pooling where pool_size == stride\n","    self.x = x.reshape(N, C, H // self.pool_size, self.pool_size,\n","                       W // self.pool_size, self.pool_size)\n","    self.out = self.x.max(axis=3).max(axis=4)\n","    # if you are returning class member be sure to return a copy\n","    return self.out.copy()\n","\n","  def backward_inputs(self, grad_out):\n","    grad_x = np.zeros_like(self.x)\n","    out_newaxis = self.out[:, :, :, np.newaxis, :, np.newaxis]\n","    mask = (self.x == out_newaxis)\n","    dout_newaxis = grad_out[:, :, :, np.newaxis, :, np.newaxis]\n","    dout_broadcast, _ = np.broadcast_arrays(dout_newaxis, grad_x)\n","    # this is almost the same as the real backward pass\n","    grad_x[mask] = dout_broadcast[mask]\n","    # in the very rare case that more then one input have the same max value\n","    # we can aprox the real gradient routing by evenly distributing across multiple inputs\n","    # but in almost all cases this sum will be 1\n","    grad_x /= np.sum(mask, axis=(3, 5), keepdims=True)\n","    grad_x = grad_x.reshape(self.input_shape)\n","    return grad_x\n","\n","\n","class Flatten(Layer):\n","  def __init__(self, input_layer, name):\n","    self.input_shape = input_layer.shape\n","    self.N = self.input_shape[0]\n","    self.num_outputs = 1\n","    for i in range(1, len(self.input_shape)):\n","      self.num_outputs *= self.input_shape[i]\n","    self.shape = (self.N, self.num_outputs)\n","    self.has_params = False\n","    self.name = name\n","\n","  def forward(self, inputs):\n","    self.input_shape = inputs.shape\n","    inputs_flat = inputs.reshape(self.input_shape[0], -1)\n","    self.shape = inputs_flat.shape\n","    return inputs_flat\n","\n","  def backward_inputs(self, grads):\n","    return grads.reshape(self.input_shape)\n","\n","\n","class FC(Layer):\n","  def __init__(self, input_layer, num_outputs, name,\n","               weights_initializer_fn=variance_scaling_initializer,\n","               bias_initializer_fn=zero_init):\n","    \"\"\"\n","    Args:\n","      input_layer: layer below\n","      num_outputs: number of neurons in this layer\n","      weights_initializer_fn: initializer function for weights,\n","      bias_initializer_fn: initializer function for biases\n","    \"\"\"\n","\n","    self.input_shape = input_layer.shape\n","    self.N = self.input_shape[0]\n","    self.shape = (self.N, num_outputs)\n","    self.num_outputs = num_outputs\n","\n","    self.num_inputs = 1\n","    for i in range(1, len(self.input_shape)):\n","      self.num_inputs *= self.input_shape[i]\n","\n","    self.weights = weights_initializer_fn([num_outputs, self.num_inputs], fan_in=self.num_inputs)\n","    self.bias = bias_initializer_fn([num_outputs])\n","    self.name = name\n","    self.has_params = True\n","\n","  def forward(self, inputs):\n","    \"\"\"\n","    Args:\n","      inputs: ndarray of shape (N, num_inputs)\n","    Returns:\n","      An ndarray of shape (N, num_outputs)\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","  def backward_inputs(self, grads):\n","    \"\"\"\n","    Args:\n","      grads: ndarray of shape (N, num_outputs)\n","    Returns:\n","      An ndarray of shape (N, num_inputs)\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","  def backward_params(self, grads):\n","    \"\"\"\n","    Args:\n","      grads: ndarray of shape (N, num_outputs)\n","    Returns:\n","      List of params and gradient pairs.\n","    \"\"\"\n","    # TODO\n","    grad_weights = ...\n","    grad_bias = ...\n","    return [[self.weights, grad_weights], [self.bias, grad_bias], self.name]\n","\n","\n","\n","class ReLU(Layer):\n","  def __init__(self, input_layer, name):\n","    self.shape = input_layer.shape\n","    self.name = name\n","    self.has_params = False\n","\n","  def forward(self, inputs):\n","    \"\"\"\n","    Args:\n","      inputs: ndarray of shape (N, C, H, W).\n","    Returns:\n","      ndarray of shape (N, C, H, W).\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","  def backward_inputs(self, grads):\n","    \"\"\"\n","    Args:\n","      grads: ndarray of shape (N, C, H, W).\n","    Returns:\n","      ndarray of shape (N, C, H, W).\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","\n","class SoftmaxCrossEntropyWithLogits():\n","  def __init__(self):\n","    self.has_params = False\n","\n","  def forward(self, x, y):\n","    \"\"\"\n","    Args:\n","      x: ndarray of shape (N, num_classes).\n","      y: ndarray of shape (N, num_classes).\n","    Returns:\n","      Scalar, average loss over N examples.\n","      It is better to compute average loss here instead of just sum\n","      because then learning rate and weight decay won't depend on batch size.\n","\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","  def backward_inputs(self, x, y):\n","    \"\"\"\n","    Args:\n","      x: ndarray of shape (N, num_classes).\n","      y: ndarray of shape (N, num_classes).\n","    Returns:\n","      Gradient with respect to the x, ndarray of shape (N, num_classes).\n","    \"\"\"\n","    # Hint: don't forget that we took the average in the forward pass\n","    # TODO\n","    pass\n","\n","\n","class L2Regularizer():\n","  def __init__(self, weights, weight_decay, name):\n","    \"\"\"\n","    Args:\n","      weights: parameters which will be regularizerized\n","      weight_decay: lambda, regularization strength\n","      name: layer name\n","    \"\"\"\n","    # this is still a reference to original tensor so don't change self.weights\n","    self.weights = weights\n","    self.weight_decay = weight_decay\n","    self.name = name\n","\n","  def forward(self):\n","    \"\"\"\n","     Returns:\n","      Scalar, loss due to the L2 regularization.\n","    \"\"\"\n","    # TODO\n","    pass\n","\n","  def backward_params(self):\n","    \"\"\"\n","    Returns:\n","      Gradient of the L2 loss with respect to the regularized weights.\n","    \"\"\"\n","    # TODO\n","    grad_weights = ...\n","    return [[self.weights, grad_weights], self.name]\n","\n","\n","class RegularizedLoss():\n","  def __init__(self, data_loss, regularizer_losses):\n","    self.data_loss = data_loss\n","    self.regularizer_losses = regularizer_losses\n","    self.has_params = True\n","    self.name = 'RegularizedLoss'\n","\n","  def forward(self, x, y):\n","    loss_val = self.data_loss.forward(x, y)\n","    for loss in self.regularizer_losses:\n","      loss_val += loss.forward()\n","    return loss_val\n","\n","  def backward_inputs(self, x, y):\n","    return self.data_loss.backward_inputs(x, y)\n","\n","  def backward_params(self):\n","    grads = []\n","    for loss in self.regularizer_losses:\n","      grads += [loss.backward_params()]\n","    return grads\n","\n"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-4095031e7bbe>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstats\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mpyximport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minstall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mim2col_cython\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcol2im_cython\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mim2col_cython\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyximport'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]},{"metadata":{"id":"X0aSdrB41_wC","colab_type":"text"},"cell_type":"markdown","source":[""]}]}